1
00:00:01,180 --> 00:00:07,459
Welcome to the unit of Hadoop Fundamentals
on Hadoop architecture.

2
00:00:07,459 --> 00:00:12,630
I will begin with a terminology review and
then cover the major components

3
00:00:12,630 --> 00:00:19,630
of Hadoop. We will see what types of nodes
can exist in a Hadoop cluster and talk about

4
00:00:19,789 --> 00:00:26,789
how Hadoop uses replication to lessen data
loss. Finally I will explain an important

5
00:00:27,140 --> 00:00:34,140
feature of Hadoop called "rack awareness"
or "network topology awareness".

6
00:00:36,210 --> 00:00:42,510
Before we examine Hadoop components and architecture,
let’s review some of the

7
00:00:42,510 --> 00:00:49,510
terms that are used in this discussion.
A node is simply a computer. This is typically

8
00:00:49,969 --> 00:00:56,969
non-enterprise, commodity hardware for
nodes that contain data. So in this example,

9
00:01:00,770 --> 00:01:07,770
we have Node 1. Then we can add more
nodes, such as Node 2, Node 3, and so on.

10
00:01:11,330 --> 00:01:17,350
This would be called a rack. A rack is a collection
of 30 or 40 nodes that are

11
00:01:17,350 --> 00:01:24,350
physically stored close together and are all
connected to the same network switch.

12
00:01:25,600 --> 00:01:29,960
Network bandwidth between any two nodes in
rack is greater than bandwidth

13
00:01:29,960 --> 00:01:36,960
between two nodes on different racks.
You will see later how Hadoop takes advantage

14
00:01:38,080 --> 00:01:43,650
of this fact.
A Hadoop Cluster (or just ‘cluster’ from

15
00:01:43,650 --> 00:01:50,650
now on) is a collection of racks
Let us now examine the pre-Hadoop 2.2 architecture.

16
00:01:56,329 --> 00:02:02,340
Hadoop has two major components:
- the distributed filesystem component, the

17
00:02:02,340 --> 00:02:08,700
main example of which is the Hadoop
Distributed File System, though other file

18
00:02:08,700 --> 00:02:15,700
systems, such as IBM GPFS-FPO, are
supported.

19
00:02:16,519 --> 00:02:22,189
- the MapReduce component, which is a framework
for performing calculations on

20
00:02:22,189 --> 00:02:29,189
the data in the distributed file system. Pre-Hadoop
2.2 MapReduce is referred to as

21
00:02:32,409 --> 00:02:39,409
MapReduce V1 and has its own built-in resource
manager and schedule. This unit covers

22
00:02:41,150 --> 00:02:47,049
the Hadoop Distributed File System and MapReduce
is covered separately.

23
00:02:47,049 --> 00:02:54,049
Let’s now examine the Hadoop distributed
file system - HDFS

24
00:02:54,279 --> 00:03:01,279
HDFS runs on top of the existing file systems
on each node in a Hadoop cluster. It is not

25
00:03:03,099 --> 00:03:09,909
POSIX compliant. It is designed to tolerate
high component failure rate through

26
00:03:09,909 --> 00:03:16,909
replication of the data.
Hadoop works best with very large files. The

27
00:03:17,069 --> 00:03:21,569
larger the file, the less time Hadoop
spends seeking for the next data location

28
00:03:21,569 --> 00:03:28,569
on disk, the more time Hadoop runs at
the limit of the bandwidth of your disks.

29
00:03:32,370 --> 00:03:37,930
Seeks are generally expensive operations
that are useful when you only need to analyze

30
00:03:37,930 --> 00:03:44,269
a small subset of your dataset. Since
Hadoop is designed to run over your entire

31
00:03:44,269 --> 00:03:51,239
dataset, it is best to minimize seeks by
using large files. Hadoop is designed for

32
00:03:51,239 --> 00:03:58,199
streaming or sequential data access rather
than random access. Sequential data access

33
00:03:58,199 --> 00:04:04,079
means fewer seeks, since Hadoop only
seeks to the beginning of each block and begins

34
00:04:04,079 --> 00:04:10,619
reading sequentially from there.
Hadoop uses blocks to store a file or parts

35
00:04:10,619 --> 00:04:16,150
of a file.
This is shown in the figure.

36
00:04:16,150 --> 00:04:23,150
Let us now examine file blocks in more detail.
A Hadoop block is a file on the underlying

37
00:04:24,620 --> 00:04:31,400
filesystem. Since the underlying
filesystem stores files as blocks, one Hadoop

38
00:04:31,400 --> 00:04:37,500
block may consist of many blocks in
the underlying file system.

39
00:04:37,500 --> 00:04:42,800
Blocks are large. They default to 64 megabytes
each and most systems

40
00:04:42,800 --> 00:04:49,800
run with block sizes of 128 megabytes or larger.
Blocks have several advantages:

41
00:04:52,500 --> 00:04:59,500
Firstly, they are fixed in size. This makes
it easy to calculate how many can fit on a

42
00:04:59,960 --> 00:05:04,189
disk.
Secondly, by being made up of blocks that

43
00:05:04,189 --> 00:05:10,080
can be spread over multiple nodes, a file
can be larger than any single disk in the

44
00:05:10,080 --> 00:05:17,080
cluster.
HDFS blocks also don't waste space. If a file

45
00:05:18,050 --> 00:05:23,500
is not an even multiple of the block
size, the block containing the remainder does

46
00:05:23,500 --> 00:05:27,620
not occupy the space of an entire
block.

47
00:05:27,620 --> 00:05:34,419
As shown in the figure, a 450 megabyte file
with a 128 megabyte block size

48
00:05:34,419 --> 00:05:41,419
consumes four blocks, but the fourth block
does not consume a full 128 megabytes.

49
00:05:43,060 --> 00:05:50,060
Finally, blocks fit well with replication,
which allows HDFS to be fault tolerant and

50
00:05:51,629 --> 00:05:58,629
available on commodity hardware.
As shown in the figure:

51
00:05:59,120 --> 00:06:06,050
Each block is replicated to multiple nodes.
For example, block 1 is stored on node 1

52
00:06:06,050 --> 00:06:13,050
and node 2. Block 2 is stored on node 1 and
node 3. And block 3 is stored on node 2 and

53
00:06:14,560 --> 00:06:21,560
node 3. This allows for node failure without
data loss. If node 1 crashes, node 2 still

54
00:06:23,449 --> 00:06:27,800
runs
and has block 1's data. In this example, we

55
00:06:27,800 --> 00:06:33,560
are only replicating data across two
nodes, but you can set replication to be across

56
00:06:33,560 --> 00:06:38,379
many more nodes by changing
Hadoop's configuration or even setting the

57
00:06:38,379 --> 00:06:45,379
replication factor for each individual file.
The second major component of Hadoop, described

58
00:06:45,979 --> 00:06:51,710
in detail in another lecture, is
the MapReduce component.

59
00:06:51,710 --> 00:06:57,840
HDFS was based on a paper Google published
about their Google File System,

60
00:06:57,840 --> 00:07:02,419
Hadoop's MapReduce is inspired by a paper
Google published on the MapReduce

61
00:07:02,419 --> 00:07:09,419
technology. It is designed to process huge
datasets for certain kinds of distributable

62
00:07:09,599 --> 00:07:16,599
problems using a large number of nodes.
A MapReduce program consists of two types

63
00:07:16,749 --> 00:07:23,749
of transformations that can be applied
to data any number of times - a map transformation

64
00:07:24,370 --> 00:07:31,370
and a reduce transformation.
A MapReduce job is an executing MapReduce

65
00:07:31,430 --> 00:07:36,590
program that is divided into map
tasks that run in parallel with each other

66
00:07:36,590 --> 00:07:41,099
and reduce tasks that run in parallel with
each other.

67
00:07:41,099 --> 00:07:48,099
Let us examine the main types of nodes in
pre-Hadoop 2.2. They are classified as HDFS

68
00:07:52,870 --> 00:07:59,870
or MapReduce V1 nodes. For HDFS nodes we have
the NameNode, and the DataNodes.

69
00:08:02,150 --> 00:08:08,539
For MapReduce V1 nodes we have the JobTracker
and the TaskTracker nodes. Each of

70
00:08:08,539 --> 00:08:15,169
these is discussed in more detail later in
this presentation. There are other HDFS

71
00:08:15,169 --> 00:08:20,449
nodes such as the Secondary NameNode, Checkpoint
node, and Backup node that

72
00:08:20,449 --> 00:08:25,590
are not discussed in this course.
This diagram shows some of the communication

73
00:08:25,590 --> 00:08:30,189
paths between the different types of
nodes on the system. A client is shown as

74
00:08:30,189 --> 00:08:36,169
communicating with a JobTracker. It can
also communicate with the NameNode and with

75
00:08:36,169 --> 00:08:42,389
any DataNode.
There is only one NameNode in the cluster.

76
00:08:42,389 --> 00:08:48,970
While the data that makes up a file is
stored in blocks at the data nodes, the metadata

77
00:08:48,970 --> 00:08:55,720
for a file is stored at the NameNode.
The NameNode is also responsible for the filesystem

78
00:08:55,720 --> 00:09:02,720
namespace. To compensate for
the fact that there is only one NameNode,

79
00:09:02,839 --> 00:09:07,870
one should configure the NameNode to
write a copy of its state information to multiple

80
00:09:07,870 --> 00:09:14,870
locations, such as a local disk and an
NFS mount. If there is one node in the cluster

81
00:09:18,089 --> 00:09:24,790
to spend money on the best enterprise
hardware for maximum reliability, it is the

82
00:09:24,790 --> 00:09:31,440
NameNode. The NameNode should also
have as much RAM as possible because it keeps

83
00:09:31,440 --> 00:09:35,880
the entire filesystem metadata in
memory.

84
00:09:35,880 --> 00:09:42,880
An typical HDFS cluster has many DataNodes.
DataNodes store the blocks of data and

85
00:09:46,740 --> 00:09:51,250
blocks from different files can be stored
on the same DataNode.

86
00:09:51,250 --> 00:09:57,899
When a client requests a file, the client
finds out from the NameNode which DataNodes

87
00:09:57,899 --> 00:10:04,899
stored the blocks that make up that file and
the client directly reads the blocks from

88
00:10:05,360 --> 00:10:10,350
the
individual DataNodes. Each DataNode also reports

89
00:10:10,350 --> 00:10:17,050
to the NameNode periodically
with the list of blocks it stores. DataNodes

90
00:10:17,050 --> 00:10:24,050
do not require expensive enterprise
hardware or replication at the hardware layer.

91
00:10:25,240 --> 00:10:30,690
The DataNodes are designed to run on
commodity hardware and replication is provided

92
00:10:30,690 --> 00:10:37,690
at the software layer.
A JobTracker node manages MapReduce V1 jobs.

93
00:10:40,820 --> 00:10:47,820
There is only one of these on the
cluster. It receives jobs submitted by clients.

94
00:10:48,649 --> 00:10:53,490
It schedules the Map tasks and Reduce
tasks on the appropriate TaskTrackers, that

95
00:10:53,490 --> 00:11:00,490
is where the data resides, in a rack-aware
manner and it monitors for any failing tasks

96
00:11:00,980 --> 00:11:06,480
that need to be rescheduled on a different
TaskTracker.

97
00:11:06,480 --> 00:11:11,190
To achieve the parallelism for your map and
reduce tasks, there are many

98
00:11:11,190 --> 00:11:18,190
TaskTrackers in a Hadoop cluster. Each TaskTracker
spawns Java Virtual Machines

99
00:11:19,300 --> 00:11:26,100
to run your map or reduce task. It communicates
with the JobTracker and reads blocks

100
00:11:26,100 --> 00:11:31,959
from DataNodes.
This lesson continues in the next video.

